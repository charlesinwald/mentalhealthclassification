{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic Modeling Depression and Suicidality.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgGX_7Fey7H5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "3140ddfa-800f-4116-d763-552b92fc120c"
      },
      "source": [
        "!pip install praw\n",
        "!pip install nltk\n",
        "!pip install pyLDAvis\n",
        "!pip install dash\n",
        "!pip install --upgrade scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/15/4bcc44271afce0316c73cd2ed35f951f1363a07d4d5d5440ae5eb2baad78/praw-7.1.0-py3-none-any.whl (152kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 18.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 14.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 51kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 81kB 10.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 92kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 8.4MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.17\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl\n",
            "Collecting prawcore<2.0,>=1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/40/b741437ce4c7b64f928513817b29c0a615efb66ab5e5e01f66fe92d2d95b/prawcore-1.5.0-py3-none-any.whl\n",
            "Collecting websocket-client>=0.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.17->praw) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (1.24.3)\n",
            "Installing collected packages: update-checker, prawcore, websocket-client, praw\n",
            "Successfully installed praw-7.1.0 prawcore-1.5.0 update-checker-0.18.0 websocket-client-0.57.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.36.2)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.1.5)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.0.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading https://files.pythonhosted.org/packages/66/89/479de0afbbfb98d1c4b887936808764627300208bb771fcd823403645a36/funcy-1.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (20.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (51.1.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.6.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=ecd29e2677256eb70ffe9e8b429ebde19c298712315727b9404ef7161371b1da\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.15 pyLDAvis-2.1.2\n",
            "Collecting dash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/17/55244363969638edd1151de0ea4aa10e6a7849b42d7d0994e3082514e19d/dash-1.18.1.tar.gz (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from dash) (1.1.2)\n",
            "Collecting flask-compress\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/7a/9c4641f975fb9daaf945dc39da6a52fd5693ab3bbc2d53780eab3b5106f4/Flask_Compress-1.8.0-py3-none-any.whl\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from dash) (4.4.1)\n",
            "Collecting dash_renderer==1.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/fe/59a322edb128ad15205002c7b81e3f5e580f6791c4a100183289e05dbfcb/dash_renderer-1.8.3.tar.gz (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 13.5MB/s \n",
            "\u001b[?25hCollecting dash-core-components==1.14.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/ab/5ffeeed41117383d02485f5b9204dcfaa074bfbb3ff2559afac7b904ad5c/dash_core_components-1.14.1.tar.gz (3.5MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5MB 28.8MB/s \n",
            "\u001b[?25hCollecting dash-html-components==1.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/ba/bb9427c62feb25bfbaf243894eeeb4e7c67a92b426ed0575a167100e436e/dash_html_components-1.1.1.tar.gz (188kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 45.3MB/s \n",
            "\u001b[?25hCollecting dash-table==4.11.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4b/de20584b7dc82dc6e572e8b596d21b1c6e39f13d19e8c9e6f1d67bed67fd/dash_table-4.11.1.tar.gz (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from dash) (0.16.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.4->dash) (2.11.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.4->dash) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.4->dash) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.4->dash) (1.0.1)\n",
            "Collecting brotli\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/d3/7c98f05b7b9103e2f3a112ba42f269c798155b3e5404fb80bb8f823aaebe/Brotli-1.0.9-cp36-cp36m-manylinux1_x86_64.whl (357kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 48.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->dash) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly->dash) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=1.0.4->dash) (1.1.1)\n",
            "Building wheels for collected packages: dash, dash-renderer, dash-core-components, dash-html-components, dash-table\n",
            "  Building wheel for dash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash: filename=dash-1.18.1-cp36-none-any.whl size=83681 sha256=552df1a927cea63be22203109a2b3bec48da6da01a402c755f96ea8ef40e7ffc\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/56/fb/79b2169ce9fcb79753ec57a16abb8f0b7750b4c63d7eb3cea9\n",
            "  Building wheel for dash-renderer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-renderer: filename=dash_renderer-1.8.3-cp36-none-any.whl size=1013945 sha256=4b9f9281d2b28246f823a716d3363ecad3118a33fbf25f5e4d3a4442abc1d5b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/2b/5f/0928a6d1b7ebf280f21a2e925f36d662e6ba83e00b82c6b6bf\n",
            "  Building wheel for dash-core-components (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-core-components: filename=dash_core_components-1.14.1-cp36-none-any.whl size=3525927 sha256=070f06c40284d5d2aa5a88c730cc59e7e2b4352d3462bc91a6be291747ae29c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/20/48/9022e1f2cb2fed4d9925370d0e17cbb3ab1164f3742d9b5e5a\n",
            "  Building wheel for dash-html-components (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-html-components: filename=dash_html_components-1.1.1-cp36-none-any.whl size=427894 sha256=24cee97a3b8d4dcee4d32448bcfe6d964d7852b6529c743ea7a083644c8dc57b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/01/46/78e4de185a8a4a2da8ba31da16c52170f036d4cebeeb6e07a2\n",
            "  Building wheel for dash-table (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-table: filename=dash_table-4.11.1-cp36-none-any.whl size=1839869 sha256=333cc19c6a7b8c6d3ce323b309ee14fcd42c2ad11bf5e570bdb1cdb0d6f8573e\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/a0/0e/1105943524ee7060c5c45a22e45c77777a3d8801f2fe49e445\n",
            "Successfully built dash dash-renderer dash-core-components dash-html-components dash-table\n",
            "Installing collected packages: brotli, flask-compress, dash-renderer, dash-core-components, dash-html-components, dash-table, dash\n",
            "Successfully installed brotli-1.0.9 dash-1.18.1 dash-core-components-1.14.1 dash-html-components-1.1.1 dash-renderer-1.8.3 dash-table-4.11.1 flask-compress-1.8.0\n",
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/aa/db462d385c56905b731403885454188683f63c86ea68900f6f7e7558b5fa/scikit_learn-0.24.0-cp36-cp36m-manylinux2010_x86_64.whl (22.2MB)\n",
            "\u001b[K     |████████████████████████████████| 22.2MB 95.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.19.5)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.0 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy3O3xhIbEIB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "66a82459-e59c-4fa9-872e-037273e6648c"
      },
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "#Praw reddit instance for gathering Reddit posts\n",
        "reddit = praw.Reddit(client_id='EtqlvWK3wzZyXg', client_secret='cEJc0Rz3KW0uX38rb6jWG3YlUo0', user_agent='googlecolab')\n",
        "subreddit=reddit.subreddit('depression')\n",
        "\n",
        "\n",
        "#Gather Reddit posts.  Posts from /r/depression are positive, /r/anxiety are\n",
        "#negative (relevant in interpreting Confusion Matrix)\n",
        "#Moderator sticky posts are discarded, as well as posts without body text (i.e images,videos)\n",
        "posts = []\n",
        "counter = 0\n",
        "counter2 = 0\n",
        "for submission in subreddit.new(limit=20000):\n",
        "    if not submission.stickied and submission.selftext:\n",
        "      counter = counter + 1\n",
        "      post = {}\n",
        "      post['title'] = submission.title\n",
        "      post['body'] = submission.selftext\n",
        "      post['depression'] = 1\n",
        "      posts.append(post)\n",
        "\n",
        "subreddit=reddit.subreddit('suicidewatch')\n",
        "for submission in subreddit.new(limit=20000):\n",
        "    if not submission.stickied and submission.selftext:\n",
        "      counter2 = counter2 + 1\n",
        "      post = {}\n",
        "      post['title'] = submission.title\n",
        "      post['body'] = submission.selftext\n",
        "      post['depression'] = 0\n",
        "      posts.append(post)\n",
        "\n",
        "print(counter)\n",
        "print(counter2)\n",
        "posts[0:3]\n",
        "df = pd.DataFrame(posts)  \n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "980\n",
            "949\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>depression</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I feel like I can’t keep pushing</td>\n",
              "      <td>Every day feels exhausting. Every day feels li...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>am i defeated?</td>\n",
              "      <td>I don't post a lot on reddit so delete this if...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>If I’m alive that must mean there’s still hope</td>\n",
              "      <td>Even if it feels bleak. Even if I know I can o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Just trying not to jump</td>\n",
              "      <td>I've just been so depressed lately and life ha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Im so tired.</td>\n",
              "      <td>I’m so tired, but can’t close my eyes to sleep...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1924</th>\n",
              "      <td>a normal day for this suicidal kid</td>\n",
              "      <td>wake up,smoke some weed to get through the day...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1925</th>\n",
              "      <td>Existential Dread</td>\n",
              "      <td>It’s hitting hard tonight. So so so many regre...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1926</th>\n",
              "      <td>Just told my best friend I’m suicidal and didn...</td>\n",
              "      <td>Yesterday was a very hard day for me. I woke u...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1927</th>\n",
              "      <td>What song plays in your head when you're daydr...</td>\n",
              "      <td>mines the sad up song smh</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1928</th>\n",
              "      <td>My girl cheating</td>\n",
              "      <td>My girl refuses to kiss me anymore, caught her...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1929 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  title  ... depression\n",
              "0                      I feel like I can’t keep pushing  ...          1\n",
              "1                                        am i defeated?  ...          1\n",
              "2        If I’m alive that must mean there’s still hope  ...          1\n",
              "3                               Just trying not to jump  ...          1\n",
              "4                                          Im so tired.  ...          1\n",
              "...                                                 ...  ...        ...\n",
              "1924                 a normal day for this suicidal kid  ...          0\n",
              "1925                                  Existential Dread  ...          0\n",
              "1926  Just told my best friend I’m suicidal and didn...  ...          0\n",
              "1927  What song plays in your head when you're daydr...  ...          0\n",
              "1928                                   My girl cheating  ...          0\n",
              "\n",
              "[1929 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5MCnkKvbI-3"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "See https://github.com/marcmuon/nlp_yelp_review_unsupervised/blob/master/notebooks/2-train_corpus_prep_and_LDA_train.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNYno_jT37sa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa75a5cf-ed54-42d3-9384-d8c613ba19e2"
      },
      "source": [
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "import spacy\n",
        "import gensim\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    print([[word for word in simple_preprocess(str(doc)) if word in stop_words] for doc in texts])\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def strip_newline(series):\n",
        "    return [review.replace('\\n','') for review in series]\n",
        "\n",
        "#convert to lowercase\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "#bigram model approximates the probability of a word given all the previous words by using only the conditional probability of one preceding word\n",
        "#https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9\n",
        "def bigrams(words, bi_min=15, tri_min=10):\n",
        "    #Group related phrases into one token for LDA\n",
        "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
        "    bigram_model = gensim.models.phrases.Phraser(bigram)\n",
        "    return bigram_model\n",
        "\n",
        "def get_corpus(df):\n",
        "    df['body'] = strip_newline(df.body)\n",
        "    words = list(sent_to_words(df.body))\n",
        "    # words = remove_stopwords(words)\n",
        "    bigram_model = bigrams(words)\n",
        "    #Get a list of lists where each list represents a post and the strings in each list are a mix of unigrams and bigrams\n",
        "    bigram = [bigram_model[post] for post in words]\n",
        "    #Mapping from word IDs to words\n",
        "    id2word = gensim.corpora.Dictionary(bigram)\n",
        "    #Filter out tokens in the dictionary by their frequency. Must appear in over 10 documents and can't be in over 35% of corpus\n",
        "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
        "    #Assign new word ids to all words, shrinking any gaps, because we just got rid of words that were too common or too rare\n",
        "    id2word.compactify()\n",
        "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
        "    return corpus, id2word, bigram\n",
        "\n",
        "#train_corpus4 = Posts converted into the bag-of-words (BoW) format = list of lists of (token_id, token_count) tuples. i.e  list of [(7, 2), (35, 2), (36, 1), (54, 1)]\n",
        "#train_id2word4 = Mapping from word IDs to words. It is used to determine the vocabulary size, we also filter extremes and compactify\n",
        "#bigram_train4 = list of lists where each list represents a post and the strings in each list are a mix of unigrams and bigrams\n",
        "train_corpus4, train_id2word4, bigram_train4 = get_corpus(df)\n",
        "bigram_train4[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['don',\n",
              " 'post',\n",
              " 'lot',\n",
              " 'on',\n",
              " 'reddit',\n",
              " 'so',\n",
              " 'delete',\n",
              " 'this',\n",
              " 'if',\n",
              " 'there',\n",
              " 'something',\n",
              " 've',\n",
              " 'missed',\n",
              " 'but',\n",
              " 'don_know',\n",
              " 'where',\n",
              " 'else',\n",
              " 'to',\n",
              " 'take',\n",
              " 'this',\n",
              " 'my',\n",
              " 'therapist',\n",
              " 'kicked',\n",
              " 'me',\n",
              " 'out',\n",
              " 'like',\n",
              " 'weeks',\n",
              " 'ago',\n",
              " 'it',\n",
              " 'not',\n",
              " 'like',\n",
              " 'we_were',\n",
              " 'making',\n",
              " 'any',\n",
              " 'progress',\n",
              " 'since',\n",
              " 'signed',\n",
              " 'up',\n",
              " 'years_ago',\n",
              " 'getting',\n",
              " 'to',\n",
              " 'the',\n",
              " 'point',\n",
              " 'got',\n",
              " 'the',\n",
              " 'classic',\n",
              " 'sympthoms',\n",
              " 'of',\n",
              " 'depression',\n",
              " 'but',\n",
              " 'lately',\n",
              " 've',\n",
              " 'not',\n",
              " 'even',\n",
              " 'been',\n",
              " 'trying',\n",
              " 'to',\n",
              " 'avoid',\n",
              " 'my',\n",
              " 'problems',\n",
              " 'nor',\n",
              " 'fix',\n",
              " 'them',\n",
              " 'they',\n",
              " 'just',\n",
              " 'kinda',\n",
              " 'stopped',\n",
              " 'mattering',\n",
              " 'but',\n",
              " 've',\n",
              " 'also',\n",
              " 'completely',\n",
              " 'stopped',\n",
              " 'thrill',\n",
              " 'seeking',\n",
              " 'don',\n",
              " 'masturbate',\n",
              " 'anymore',\n",
              " 'tmi',\n",
              " 'don',\n",
              " 'do',\n",
              " 'my',\n",
              " 'drugs',\n",
              " 'which',\n",
              " 've_been',\n",
              " 'heavily',\n",
              " 'addicted',\n",
              " 'to',\n",
              " 'grams',\n",
              " 'of',\n",
              " 'hash',\n",
              " 'every',\n",
              " 'fucking',\n",
              " 'day',\n",
              " 'sleep',\n",
              " 'like',\n",
              " 'not',\n",
              " 'at',\n",
              " 'all',\n",
              " 'well',\n",
              " 'go',\n",
              " 'to',\n",
              " 'bed',\n",
              " 'am',\n",
              " 'and',\n",
              " 'wake_up',\n",
              " 'around',\n",
              " 'pm',\n",
              " 'now',\n",
              " 'that',\n",
              " 'out',\n",
              " 'of',\n",
              " 'work',\n",
              " 'but',\n",
              " 'even',\n",
              " 'when',\n",
              " 'did',\n",
              " 'work',\n",
              " 'it',\n",
              " 'wasn',\n",
              " 'all',\n",
              " 'the',\n",
              " 'diffrent',\n",
              " 'except',\n",
              " 'go',\n",
              " 'to',\n",
              " 'bed',\n",
              " 'at',\n",
              " 'am',\n",
              " 'and',\n",
              " 'get',\n",
              " 'up',\n",
              " 'at',\n",
              " 'am',\n",
              " 'got',\n",
              " 'replaced',\n",
              " 'by',\n",
              " 'someone',\n",
              " 'my',\n",
              " 'boss',\n",
              " 'doesn',\n",
              " 'need',\n",
              " 'to',\n",
              " 'pay',\n",
              " 'for',\n",
              " 'the',\n",
              " 'new',\n",
              " 'guy',\n",
              " 'gets',\n",
              " 'his',\n",
              " 'salery',\n",
              " 'through',\n",
              " 'welfare',\n",
              " 've_tried',\n",
              " 'my',\n",
              " 'fair',\n",
              " 'share',\n",
              " 'of',\n",
              " 'antidepressants',\n",
              " 'didn',\n",
              " 'really',\n",
              " 'work',\n",
              " 'for',\n",
              " 'me',\n",
              " 'at',\n",
              " 'all',\n",
              " 'feel',\n",
              " 'im',\n",
              " 'wasting',\n",
              " 'my_own',\n",
              " 'time',\n",
              " 'by',\n",
              " 'being_alive',\n",
              " 'but',\n",
              " 'don',\n",
              " 'belive',\n",
              " 'in',\n",
              " 'an',\n",
              " 'afterlife',\n",
              " 'or',\n",
              " 'anything',\n",
              " 'like',\n",
              " 'that',\n",
              " 'just',\n",
              " 'can',\n",
              " 'convince',\n",
              " 'myself',\n",
              " 'the',\n",
              " 'effort',\n",
              " 'will_be',\n",
              " 'worth',\n",
              " 'it',\n",
              " 'without',\n",
              " 'peaking',\n",
              " 'over',\n",
              " 'the',\n",
              " 'fencei',\n",
              " 'feel',\n",
              " 'want',\n",
              " 'nothing',\n",
              " 'and',\n",
              " 'that',\n",
              " 'there',\n",
              " 'no_reason',\n",
              " 'to',\n",
              " 'be',\n",
              " 'here',\n",
              " 'don',\n",
              " 'have',\n",
              " 'better',\n",
              " 'reason',\n",
              " 'than',\n",
              " 'it',\n",
              " 'make',\n",
              " 'people',\n",
              " 'sad',\n",
              " 'and',\n",
              " 'all',\n",
              " 'of',\n",
              " 'them',\n",
              " 'tell',\n",
              " 'me',\n",
              " 'life',\n",
              " 'is',\n",
              " 'very',\n",
              " 'awesome',\n",
              " 'but',\n",
              " 'tbh',\n",
              " 'don',\n",
              " 'think',\n",
              " 'll',\n",
              " 'care',\n",
              " 'much',\n",
              " 'after',\n",
              " 'the_fact']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shT_HECepdGu"
      },
      "source": [
        "# LDA Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APBOscpNyuwr"
      },
      "source": [
        "# Determine number of topics to use with coherence scores\n",
        "\n",
        "**Measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic.**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "The number of topics to use in the LDA topic modeling is chosen at runtime, so \n",
        "this may take a while to run, especially with high volume input data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgH_i4tKpcMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b656e9-ead7-46da-b7da-59e7aeb8fa22"
      },
      "source": [
        "import logging\n",
        "import warnings\n",
        "from gensim.models import CoherenceModel\n",
        "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "topicnum = 1\n",
        "besttopicnum = 1\n",
        "bestcoherencescore = 0\n",
        "for topicnum in range(1,50):\n",
        "  print(topicnum)\n",
        "    #Make LDA model\n",
        "    #Model file saved in same directory\n",
        "  with warnings.catch_warnings():\n",
        "      warnings.simplefilter('ignore')\n",
        "      lda_train4 = gensim.models.ldamulticore.LdaMulticore(\n",
        "                                corpus=train_corpus4,\n",
        "                                num_topics=topicnum,\n",
        "                                id2word=train_id2word4,\n",
        "                                chunksize=100, #number of documents to consider at once (affects the memory consumption)\n",
        "                                workers=7, # Num. Processing Cores - 1\n",
        "                                random_state=42,\n",
        "                                passes=50,\n",
        "                                eval_every = 1,\n",
        "                                per_word_topics=True)\n",
        "      #Print top 15 words for each of the topics\n",
        "      # print(lda_train4.print_topics(topicnum,num_words=15))\n",
        "      lda_train4.save('lda_train4.model')\n",
        "      \n",
        "      #Calculate coherence score\n",
        "      coherence_model_lda = CoherenceModel(model=lda_train4, texts=bigram_train4, dictionary=train_id2word4, coherence='c_v')\n",
        "      coherence_lda = coherence_model_lda.get_coherence()\n",
        "      print('coherence score: ')\n",
        "      print(coherence_lda)\n",
        "      if coherence_lda > bestcoherencescore:\n",
        "        bestcoherencescore = coherence_lda\n",
        "        besttopicnum = topicnum\n",
        "\n",
        "topicnum = besttopicnum\n",
        "print('Chosen number of topics and Coherence Score')\n",
        "print(topicnum)\n",
        "print(bestcoherencescore)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "coherence score: \n",
            "0.22118790219010537\n",
            "2\n",
            "coherence score: \n",
            "0.2823887057600245\n",
            "3\n",
            "coherence score: \n",
            "0.27988015762311685\n",
            "4\n",
            "coherence score: \n",
            "0.29242021150240954\n",
            "5\n",
            "coherence score: \n",
            "0.3031085668052255\n",
            "6\n",
            "coherence score: \n",
            "0.2962822541567607\n",
            "7\n",
            "coherence score: \n",
            "0.28367672643335823\n",
            "8\n",
            "coherence score: \n",
            "0.30018420538927665\n",
            "9\n",
            "coherence score: \n",
            "0.28443112809650767\n",
            "10\n",
            "coherence score: \n",
            "0.3108076259082785\n",
            "11\n",
            "coherence score: \n",
            "0.28691981203937517\n",
            "12\n",
            "coherence score: \n",
            "0.2845007377527376\n",
            "13\n",
            "coherence score: \n",
            "0.2876892940805057\n",
            "14\n",
            "coherence score: \n",
            "0.29651028023524306\n",
            "15\n",
            "coherence score: \n",
            "0.29367111786093664\n",
            "16\n",
            "coherence score: \n",
            "0.29008593572052244\n",
            "17\n",
            "coherence score: \n",
            "0.2853231586942488\n",
            "18\n",
            "coherence score: \n",
            "0.291584330425495\n",
            "19\n",
            "coherence score: \n",
            "0.2850613131162891\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdvWeP6E6cdM"
      },
      "source": [
        "# Convert Topics to Feature Vectors\n",
        "\n",
        "\n",
        "*   Get distribution of the n topics for every post\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBv_8gUg6b95"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "#Make Vectors\n",
        "train_vecs = []\n",
        "transformer = FunctionTransformer(np.log1p, validate=True)\n",
        "# topicnum = 9\n",
        "for i in range(len(df)):\n",
        "    top_topics = lda_train4.get_document_topics(train_corpus4[i], minimum_probability=0.0)\n",
        "    top_topics = transformer.transform(top_topics)\n",
        "    topic_vec = preprocessing.scale([top_topics[i][1] for i in range(topicnum)]).tolist()\n",
        "    # topic_vec = transformer.transform(topic_vec)\n",
        "    # topic_vec.extend([df.iloc[i].real_counts]) # counts of posts for poster?\n",
        "    topic_vec.extend([len(df.iloc[i].body)]) # length of post\n",
        "    train_vecs.append(topic_vec)\n",
        "\n",
        "train_vecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_-H2Cju9FEQ"
      },
      "source": [
        "# Train Classifier\n",
        "\n",
        "\n",
        "*   Logistic Regression\n",
        "*   Stochastic Gradient Descent (SGD) with Log Loss\n",
        "*   SGD with Modified Huber Loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an7CXS9R9E0R"
      },
      "source": [
        "#Sklearn needs NumPy arrays\n",
        "import numpy as np\n",
        "\n",
        "X = np.array(train_vecs)\n",
        "\n",
        "#For now we are using the karma score as the target, should be changed later to suicidality risk score\n",
        "y = np.array(df.depression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5psj3ki92Rfh"
      },
      "source": [
        "# **Model Selection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqH8Sfsep-67"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import linear_model\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import seaborn as sns\n",
        "%config InlineBackend.figure_formats = ['retina']\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cmap\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.externals.joblib import parallel_backend\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    # ('clf', linear_model.SGDClassifier()),\n",
        "    ('clf', linear_model.LogisticRegression()),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "parameters = {\n",
        "    #SGD ONLY\n",
        "    # 'clf__alpha': (1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3),\n",
        "    # 'clf__loss': ('hinge','log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'), \n",
        "    # 'clf__penalty': ('l1','l2', 'elasticnet','none'),\n",
        "    #LR ONLY\n",
        "    'clf__C': np.logspace(0, 4, 10),\n",
        "    'clf__solver': ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'),\n",
        "\n",
        "    'clf__max_iter': (10, 20, 50, 80, 1000, 10000, 100000),\n",
        "}\n",
        "# Split the dataset in two equal parts\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.5, random_state=0)\n",
        "\n",
        "scores = ['precision', 'recall']\n",
        "\n",
        "for score in scores:\n",
        "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
        "    print()\n",
        "\n",
        "    # clf = GridSearchCV(\n",
        "    #     linear_model.LogisticRegression(), hyperparameters, scoring='%s_macro' % score\n",
        "    #)\n",
        "    clf = GridSearchCV(\n",
        "        pipeline, parameters, scoring='%s_macro' % score\n",
        "    )\n",
        "    with parallel_backend('threading'):\n",
        "      clf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Best parameters set found on development set:\")\n",
        "    print()\n",
        "    print(clf.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores on development set:\")\n",
        "    print()\n",
        "    means = clf.cv_results_['mean_test_score']\n",
        "    stds = clf.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "              % (mean, std * 2, params))\n",
        "    print()\n",
        "\n",
        "    print(\"Detailed classification report:\")\n",
        "    print()\n",
        "    print(\"The model is trained on the full development set.\")\n",
        "    print(\"The scores are computed on the full evaluation set.\")\n",
        "    print()\n",
        "    y_true, y_pred = y_test, clf.predict(X_test)\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVS_dW2sTdek"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "base_learners = [\n",
        "                 ('rf_1', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
        "                 ('rf_2', KNeighborsClassifier(n_neighbors=5))             \n",
        "                ]\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "clf = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression(), cv=loo)\n",
        "# Extract score\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "print(clf.score(X_test, y_test))\n",
        "y_true = clf.predict(X_train)\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "# print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u05mB2D-Al0Q"
      },
      "source": [
        "\n",
        "# Plot a given confusion matrix, should be calculated beforehand using scikit's \n",
        "# confusion matrix, and averaged with numpy mean across columns due to the kfold\n",
        "def plot_confusion_matrix(cm,title):\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap.Blues)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    classes=['depression', 'anxiety']\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' #if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "\n",
        "\n",
        "\n",
        "# Cross Validate - The 5 folds take turns being the validation set while the others\n",
        "# form the training set:\n",
        "\n",
        "    # Shuffle the dataset randomly.\n",
        "    # Split the dataset into k groups\n",
        "    # For each unique group:\n",
        "    #     Take the group as a hold out or test data set\n",
        "    #     Take the remaining groups as a training data set\n",
        "    #     Fit a model on the training set and evaluate it on the test set\n",
        "    #     Retain the evaluation score and discard the model\n",
        "    # Summarize the skill of the model using the sample of model evaluation scores\n",
        "\n",
        "kf = KFold(5, shuffle=True, random_state=42)\n",
        "# F1 scores\n",
        "cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1  = [], [], []\n",
        "# Confusion Matrices\n",
        "cv_lr_cm, cv_lrsgd_cm, cv_svcsgd_cm  = [], [], []\n",
        "\n",
        "# For each of the 5 folds\n",
        "for train_ind, val_ind in kf.split(X, y):\n",
        "    # Assign CV Indices\n",
        "    X_train, y_train = X[train_ind], y[train_ind]\n",
        "    X_val, y_val = X[val_ind], y[val_ind]\n",
        "    \n",
        "\n",
        "\n",
        "    # Scale Data down to 1/5 of the total size because of the folds\n",
        "    scaler = StandardScaler()\n",
        "    # Fit to data, then transform it with centering and scaling\n",
        "    X_train_scale = scaler.fit_transform(X_train)\n",
        "    X_val_scale = scaler.transform(X_val)\n",
        "\n",
        "    # Logisitic Regression\n",
        "    lr = LogisticRegression(\n",
        "        class_weight= 'balanced',\n",
        "        solver='newton-cg',\n",
        "        fit_intercept=True,\n",
        "        C=166.81005372000593,\n",
        "        max_iter=20,\n",
        "    ).fit(X_train_scale, y_train)\n",
        "\n",
        "    y_pred = lr.predict(X_val_scale)\n",
        "    cv_lr_f1.append(f1_score(y_val, y_pred, average='micro'))\n",
        "    cv_lr_cm.append(confusion_matrix(y_val,y_pred))\n",
        "    # # Logistic Regression Mini-Batch SGD\n",
        "    # sgd = linear_model.SGDClassifier(\n",
        "    #     max_iter=1000,\n",
        "    #     tol=1e-3,\n",
        "    #     loss='log',\n",
        "    #     class_weight='balanced'\n",
        "    # ).fit(X_train_scale, y_train)\n",
        "    \n",
        "    # y_pred = sgd.predict(X_val_scale)\n",
        "    # cv_lrsgd_f1.append(f1_score(y_val, y_pred, average='micro'))\n",
        "    # cv_lrsgd_cm.append(confusion_matrix(y_val,y_pred))\n",
        "\n",
        "\n",
        "    # # SGD Modified Huber\n",
        "    # sgd_huber = linear_model.SGDClassifier(\n",
        "    #     max_iter=1000,\n",
        "    #     tol=1e-3,\n",
        "    #     alpha=1,\n",
        "    #     loss='modified_huber',\n",
        "    #     class_weight='balanced'\n",
        "    # ).fit(X_train_scale, y_train)\n",
        "    \n",
        "    # y_pred = sgd_huber.predict(X_val_scale)\n",
        "    # cv_svcsgd_f1.append(f1_score(y_val, y_pred, average='micro'))\n",
        "    # cv_svcsgd_cm.append(confusion_matrix(y_val,y_pred))\n",
        "\n",
        "\n",
        "print(f'Logistic Regression Val f1: {np.mean(cv_lr_f1):.3f} +- {np.std(cv_lr_f1):.3f}')\n",
        "print(f'Logisitic Regression SGD Val f1: {np.mean(cv_lrsgd_f1):.3f} +- {np.std(cv_lrsgd_f1):.3f}')\n",
        "print(f'Huber Val f1: {np.mean(cv_svcsgd_f1):.3f} +- {np.std(cv_svcsgd_f1):.3f}')\n",
        "\n",
        "cm = np.mean(cv_lr_cm, axis=0)\n",
        "plot_confusion_matrix(cm,'Logistic Regression')\n",
        "# print(f'Logistic Regression Val Confusion Matrix: {cv_lr_cm}')\n",
        "# cm = np.mean(cv_lrsgd_cm, axis=0)\n",
        "# plot_confusion_matrix(cm,'Logistic Regression SGD')\n",
        "# cm = np.mean(cv_svcsgd_cm, axis=0)\n",
        "# plot_confusion_matrix(cm,'SVM Huber')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh1PJN7V2K_2"
      },
      "source": [
        "Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmsmQFk38uDK"
      },
      "source": [
        "# Legend (Depression is positive, anxiety is negative)\n",
        "![Confusion Matrix Legend](https://i.stack.imgur.com/ysM0Z.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CmLgjT-u4eu"
      },
      "source": [
        "# Save Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kZlKgZHu2cR"
      },
      "source": [
        "import pickle\n",
        "with open('train_corpus4.pkl', 'wb') as f:\n",
        "    pickle.dump(train_corpus4, f)\n",
        "with open('train_id2word4.pkl', 'wb') as f:\n",
        "    pickle.dump(train_id2word4, f)\n",
        "with open('bigram_train4.pkl', 'wb') as f:\n",
        "    pickle.dump(bigram_train4, f)\n",
        "# with open('lemma_train.pkl', 'wb') as f:\n",
        "#     pickle.dump(lemma_train, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqKKVmGHsReE"
      },
      "source": [
        "# Test on unseen data\n",
        "\n",
        "\n",
        "*   Preprocess test data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbh4GkxcqwVw"
      },
      "source": [
        "subreddit=reddit.subreddit('depression')\n",
        "\n",
        "\n",
        "#Create Pandas dataframe from list of dictionaries\n",
        "posts = []\n",
        "for submission in subreddit.new(limit=100):\n",
        "    if not submission.stickied and submission.selftext:\n",
        "      post = {}\n",
        "      post['title'] = submission.title\n",
        "      post['body'] = submission.selftext\n",
        "      post['depression'] = 1\n",
        "      posts.append(post)\n",
        "\n",
        "subreddit=reddit.subreddit('happy')\n",
        "for submission in subreddit.new(limit=100):\n",
        "    if not submission.stickied and submission.selftext:\n",
        "      post = {}\n",
        "      post['title'] = submission.title\n",
        "      post['body'] = submission.selftext\n",
        "      post['depression'] = 0\n",
        "      posts.append(post)\n",
        "\n",
        "\n",
        "posts[0:3]\n",
        "df = pd.DataFrame(posts)  \n",
        "df\n",
        "# We only need the bigram_train5 but we are using the the earlier defined function so we ignore the first 2 returned values\n",
        "_, _, bigram_test = get_corpus(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7bkcV2Gtzkg"
      },
      "source": [
        "Load LDA Model and ID to word mapping from earlier from filesystem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER1FglJ-tvrd"
      },
      "source": [
        "import smart_open\n",
        "\n",
        "lda_train4 = gensim.models.ldamulticore.LdaMulticore.load('lda_train4.model')\n",
        "with open('train_id2word4.pkl', 'rb') as f:\n",
        "    train_id2word4 = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOLtHogMxjFs"
      },
      "source": [
        "#Use training dictionary on the new words\n",
        "test_corpus = [train_id2word4.doc2bow(text) for text in bigram_test]\n",
        "\n",
        "\n",
        "test_vecs = []\n",
        "for i in range(len(df)):\n",
        "    top_topics = lda_train4.get_document_topics(test_corpus[i], minimum_probability=0.0)\n",
        "    topic_vec = [top_topics[i][1] for i in range(topicnum)]\n",
        "    topic_vec.extend([len(df.iloc[i].body)])\n",
        "    test_vecs.append(topic_vec)\n",
        "\n",
        "X = np.array(test_vecs)\n",
        "y = np.array(df.depression)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c1k3UMhuHKs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwsTVqcuyjPb"
      },
      "source": [
        "ss = StandardScaler()\n",
        "X = ss.fit_transform(X)\n",
        "\n",
        "# lr = LogisticRegression(\n",
        "#   class_weight= 'balanced',\n",
        "#   solver='newton-cg',\n",
        "#   fit_intercept=True\n",
        "#   ).fit(X, y)\n",
        "\n",
        "y_pred_lr = lr.predict(X)\n",
        "y_pred_lr\n",
        "\n",
        "for i in range(len(df)):\n",
        "  print(df.iloc[i].body)\n",
        "  print(y_pred_lr[i])\n",
        "  print('\\n')\n",
        "# print(f1_score(y, y_pred_lr,average='micro'))\n",
        "\n",
        "# sgd_huber = linear_model.SGDClassifier(\n",
        "#         max_iter=1000,\n",
        "#         tol=1e-3,\n",
        "#         alpha=20,\n",
        "#         loss='modified_huber',\n",
        "#         class_weight='balanced',shuffle=True\n",
        "#     ).fit(X, y)\n",
        "    \n",
        "# y_pred_huber = sgd_huber.predict(X)\n",
        "# print(f1_score(y, y_pred_huber, average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}